"vettel" = 1,
"wehrlein" = 2
)
p + geom_line() +
scale_y_reverse() +
scale_linetype_manual(values= drivers2) +
scale_colour_manual(values = drivers)
p + geom_line() +
scale_y_reverse() +
scale_linetype_manual(values= drivers2) +
scale_colour_manual(values = factor(drivers))
p + geom_line() +
scale_y_reverse() +
scale_linetype_manual(values= factor(drivers2)) +
scale_colour_manual(values = drivers)
p + geom_line() +
scale_y_reverse() +
scale_linetype_discrete(values= factor(drivers2)) +
scale_colour_manual(values = drivers)
p + geom_line() +
scale_y_reverse() +
scale_linetype_discrete() +
scale_colour_manual(values = drivers)
shiny::runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
palette <- colorRampPalette(magma,100)
library(viridis)
palette <- colorRampPalette(magma,100)
palette <- colorRampPalette(magma,1:100)
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
lap_data <- fromJSON(paste0("/Users/jaocbwarner/Desktop/F1/ergast_f1/lap_results/2017_7.json"))
df <- data.frame(lap_data$MRData$RaceTable$Races$Laps[[1]][[2]][[1]][1])
df <- data.frame(df[order(df$driverId),])
names(df) <- c("driverId")
timeout <- lapply(seq_along(lap_data$MRData$RaceTable$Races$Laps[[1]][[1]]), function(i){
driver <- lap_data$MRData$RaceTable$Races$Laps[[1]][[2]][[i]][1]
time <- lap_data$MRData$RaceTable$Races$Laps[[1]][[2]][[i]][3]
names(time) <- c(i)
x <- cbind(driver,time)
x <- x[order(x[1]),]
df <- merge(df,x, by.x="driverId", by.y = "driverId", all=T)
df[2]
})
timeoutf <- do.call(cbind,timeout)
timeoutf <- cbind(df,timeoutf)
IDS <- DriverData[match(timeoutf$driverId,DriverData$X2),][,1]
IDS <- strsplit(as.character(IDS),',')
timeoutf$driverId <- unlist(lapply(IDS, '[', 1))
rownames(timeoutf) <- timeoutf$driverId
drivers <- fromJSON("http://ergast.com/api/f1/drivers/.json?limit=2500")
parsedrivers= data.frame(name=character())
alldriversresults <- lapply(1:as.numeric(drivers[[1]][6][1]), function(i){
id <- drivers[[1]][[7]][[2]][[1]][i]
firstname <- drivers[[1]][[7]][[2]][[3]][i]
lastname <- toupper(drivers[[1]][[7]][[2]][[4]][i])
parsedrivers=rbind(parsedrivers,data.frame(
paste0(lastname,", ",firstname),
id)
)
names(parsedrivers) = NULL
parsedrivers
})
AllDrivers <- unlist(lapply(alldriversresults, '[[', 1))
DriverData <- do.call(rbind, lapply(alldriversresults, data.frame, stringsAsFactors=FALSE))
df <- data.frame(lap_data$MRData$RaceTable$Races$Laps[[1]][[2]][[1]][1])
df <- data.frame(df[order(df$driverId),])
names(df) <- c("driverId")
timeout <- lapply(seq_along(lap_data$MRData$RaceTable$Races$Laps[[1]][[1]]), function(i){
driver <- lap_data$MRData$RaceTable$Races$Laps[[1]][[2]][[i]][1]
time <- lap_data$MRData$RaceTable$Races$Laps[[1]][[2]][[i]][3]
names(time) <- c(i)
x <- cbind(driver,time)
x <- x[order(x[1]),]
df <- merge(df,x, by.x="driverId", by.y = "driverId", all=T)
df[2]
})
timeoutf <- do.call(cbind,timeout)
timeoutf <- cbind(df,timeoutf)
IDS <- DriverData[match(timeoutf$driverId,DriverData$X2),][,1]
IDS <- strsplit(as.character(IDS),',')
timeoutf$driverId <- unlist(lapply(IDS, '[', 1))
rownames(timeoutf) <- timeoutf$driverId
timeoutf <- timeoutf[-c(1)]
laptimes_s<- lapply(seq_along(row.names(timeoutf)), function(y){
sapply(seq_along(timeoutf),function(i){
timeInS(timeoutf[y,i])
})
})
getNum=function(x){as.numeric(as.character(x))}
timeInS=function(tStr){
if (is.na(tStr)) tS=NA
else {
x=unlist(strsplit(tStr,':'))
if (length(x)==1) tS=getNum(x[1])
else if (length(x)==2) tS=60*getNum(x[1])+getNum(x[2])
else if (length(x)==3) tS=3600*getNum(x[1])+60*getNum(x[2])+getNum(x[3])
else tS=NA
}
tS
}
laptimes_s<- lapply(seq_along(row.names(timeoutf)), function(y){
sapply(seq_along(timeoutf),function(i){
timeInS(timeoutf[y,i])
})
})
laptimes_s <- do.call(rbind,laptimes_s)
View(laptimes_s)
means <- colMeans(laptimes_s, na.rm = T)
delta_times <- lapply(1:length(laptimes_s[,1]), function(y){
sapply(1:length(laptimes_s[1,]),function(i){
laptimes_s[y,i] <- ( means[y]/ laptimes_s[y,i])
})
})
delta_times <- do.call(rbind,delta_times)
colnames(delta_times) <- c(1:length(laptimes_s[1,]))
rownames(delta_times) <- row.names(timeoutf)
View(delta_times)
runApp('Desktop/F1/ergast_f1')
View(laptimes_s)
quantile.range <- quantile(laptimes_s, probs = seq(0, 1, 0.01))
quantile.range <- quantile(laptimes_s, probs = seq(0, 1, 0.01),na.rm=T)
quantile.range
palette.breaks <- seq(quantile.range["0%"], quantile.range["80%"], 0.1)
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
my_palette <- colorRampPalette(c("#fcf9c2","#bf3975","#000006"),100)
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
seq(quantile.range["0%"], quantile.range["80%"], 0.1)
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
runApp('Desktop/F1/ergast_f1')
shiny::runApp('Desktop/F1/ergast_f1')
runApp('Desktop/NvERTx_4')
shiny::runApp('Desktop/F1/ergast_f1')
stockhistoricals <- function(stocklist="GOOG", start_date="1970-01-01", end_date=as.Date(Sys.time()), verbose=TRUE){
## Usage stockhistoricals(stocklist, start_date, end_date, verbose=TRUE)
##    stocklist : A vector of stock tickers. Default = GOOG. Can also pass "NYSE", "NASDAQ", or "AMEX" to get those lists.
##    start_date : start date in format Year-month-day. Default = "1970-01-01"
##    end_date : end date in format Year-month-day. Default = system date
##    verbost : Logical. if F, suppresses messages
require(XML)
require(RCurl)
require(httr)
require(readr)
options(warn=-1)
if(stocklist=="NASDAQ"){
NASDAQ <- read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download")
stocklist <- NASDAQ$Symbol
} else if (stocklist == "NYSE"){
NYSE <- read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download")
stocklist <- NYSE$Symbol
} else if (stocklist == "AMEX"){
AMEX <- read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=amex&render=download")
stocklist <- AMEX$Symbol
}
options(warn=0)
#86400 is the conversion from days to minutes (how yahoo is counting time)
#t=1 is 1970-01-01. So any is 86400 minutes per day from 1970-01-01
start <- as.numeric(as.Date(start_date)-as.Date("1970-01-01"))* 86400
end <- as.numeric(as.Date(end_date)-as.Date("1970-01-01"))* 86400
#grab a cookie. Try 5 times because sometimes yahoo puts fucking escape characters in the crumb
tries = 1
status = 1
while (tries < 5 && status !=200){
url <- paste0("https://finance.yahoo.com/quote/GOOG/history")
h <- handle(url)
res <- GET(handle = h)
response <- content(res, "text")
cookie <- unlist(strsplit(unlist(res$headers[5]),';'))[1]
#this gets a crumb pair to use. I hate regex
crumbled = stringr::str_extract(response, '\"CrumbStore\\\":\\{\\\"crumb\\\":\\\"[[:graph:]]+?\\\"\\}')
crumb <- unlist(strsplit(crumbled,split='"'))[6]
#test them
testurl <- paste0("https://query1.finance.yahoo.com/v7/finance/download/GOOG?period1=1451606400&period2=1483228800&interval=1d&events=history&crumb=",crumb)
scraped <- GET(testurl, config(cookie= cookie))
status <- status_code(scraped)
tries = tries + 1
}
if (status != 200){
message("ERROR: Couldn't access Yahoo after 5 tries")
}
if (status == 401){
message("ERROR: The cookie/crumb scrape didn't work... Fucking yahoo...")
}
if (verbose == TRUE){
message("Grabbing Stock data... This takes a while for long stocklists")
}
stocksdf<- lapply(stocklist,function(x){
if (verbose == TRUE){
message(paste0("Downloading ",x))
}
capture <- paste0("https://query1.finance.yahoo.com/v7/finance/download/",x,"?period1=",start,"&period2=",end,"&interval=1d&events=history&crumb=",crumb)
scraped <- GET(capture, config(cookie= cookie))
#the content() call is loud so I suppress messages for now
suppressMessages(data.frame(content(scraped)))
})
names(stocksdf) <- stocklist
return(stocksdf)
}
AMEX <- stockhistoricals("AMEX", start_date = "2016-09-10")
names(AMEX)
sapply(names(AMEX), function(X){AMEX$x$Low})
AMEX[[1]]
AMEX[[1]][[1]]
AMEX[[1]][[2]]
AMEX[[1]][[3]]
AMEX[[1]][[4]]
AMEX[[1]][[5]]
AMEX[[1]]
AMEX[[1]][[6]]
sapply(names(AMEX), function(X){AMEX[[x]][[6]]})
sapply(names(AMEX), function(x){AMEX[[x]][[6]]})
sapply(1:length(AMEX), function(x){AMEX[[x]][[6]]})
length(AMEX)
sapply(1:length(AMEX), function(x){AMEX[[x]][[5]]})
lapply(1:length(AMEX), function(x){AMEX[[x]][[5]]})
sapply(AMEX, '[[',2)
sapply(AMEX, '[[',1)
sapply(AMEX, '[[',2)
sapply(AMEX, '[[',3)
sapply(AMEX, '[[',4)
sapply(AMEX, '[[',5)
sapply(AMEX, '[[',6)
sapply(AMEX, '[[',1)
AMEX$FAX$Low
sapply(AMEX, '[',1)
sapply(AMEX, '[',2)
sapply(AMEX, '[',3)
sapply(AMEX, '[[',3)
non.null.list <- lapply(AMEX, Filter, f = Negate(is.null))
AMEX$`ABE           `
AMEX$ABE
help(Negate)
length(AMEX$ABE)
length(AMEX$ACU)
AMEX[lapply(AMEX,length)<7]
AMEX[lapply(AMEX,length)==7]
AMEXfilt <- AMEX[lapply(AMEX,length)==7]
sapply(AMEXfilt, '[[',6)
AdjClose <- sapply(AMEXfilt, '[[',6)
AdjClose <- data.frame(sapply(AMEXfilt, '[[',6))
df <- data.frame()
AdjClose <- lapply(AMEXfilt, function(i){
close <- AMEXfilt[[i]][[6]]
df<- rbind(df, close)
df
})
AdjClose <- lapply(1:length(AMEXfilt), function(i){
close <- AMEXfilt[[i]][[6]]
df<- rbind(df, close)
df
})
AdjClose <- lapply(1:length(AMEXfilt), function(i){
close <- AMEXfilt[[i]][[6]]
df<- cbind(df, close)
df
})
do.call()
help(do.call)
AdjClose <- lapply(1:length(AMEXfilt), function(i){
close <- AMEXfilt[[i]][[6]]
df<- do.call(rbind, close)
df
})
df<- do.call(rbind, AdjClose)
df<- do.call(cbind, AdjClose)
View(df)
AdjClose <- sapply(AMEXfilt, '[[',6)
df<- do.call(cbind, AdjClose)
View(df)
df<- do.call(rbind, AdjClose)
View(df)
get_stocklists <- function(exchange="NYSE"){
if(exchange=="NASDAQ"){
read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download")
} else if (exchange == "NYSE"){
read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download")
} else if (exchange == "AMEX"){
read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=amex&render=download")
} else {
message("ERROR: a valid exchange wasn't specified.")
message("Try 'NYSE', 'NASDQ', or 'AMEX'")
message("usage: get_stocklists(exchange)")
}
}
get_stocklists()
get_stocklists("Huffpo")
get_stocklists <- function(exchange="NYSE"){
if(exchange=="NASDAQ"){
read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download")
} else if (exchange == "NYSE"){
read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download")
} else if (exchange == "AMEX"){
read.csv(file="http://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=amex&render=download")
} else {
message("ERROR: a valid exchange wasn't specified.")
message("Try 'NYSE', 'NASDQ', or 'AMEX'")
message("usage: get_stocklists(exchange)")
}
}
stockhistoricals <- function(stocklist="GOOG", start_date="1970-01-01", end_date=as.Date(Sys.time()), verbose=TRUE){
## Usage stockhistoricals(stocklist, start_date, end_date, verbose=TRUE)
##    stocklist : A vector of stock tickers. Default = GOOG. Can also pass "NYSE", "NASDAQ", or "AMEX" to get those lists.
##    start_date : start date in format Year-month-day. Default = "1970-01-01"
##    end_date : end date in format Year-month-day. Default = system date
##    verbost : Logical. if F, suppresses messages
require(XML)
require(RCurl)
require(httr)
require(readr)
options(warn=-1)
if(stocklist=="NASDAQ" | stocklist=="NYSE" | stocklist=="AMEX"){
stocks <- get_stocklists(stocklist)
stocks <- stocks$Symbol
}
options(warn=0)
#86400 is the conversion from days to minutes (how yahoo is counting time)
#t=1 is 1970-01-01. So any is 86400 minutes per day from 1970-01-01
start <- as.numeric(as.Date(start_date)-as.Date("1970-01-01"))* 86400
end <- as.numeric(as.Date(end_date)-as.Date("1970-01-01"))* 86400
#grab a cookie. Try 5 times because sometimes yahoo puts fucking escape characters in the crumb
tries = 1
status = 1
while (tries < 5 && status !=200){
url <- paste0("https://finance.yahoo.com/quote/GOOG/history")
h <- handle(url)
res <- GET(handle = h)
response <- content(res, "text")
cookie <- unlist(strsplit(unlist(res$headers[5]),';'))[1]
#this gets a crumb pair to use. I hate regex
crumbled = stringr::str_extract(response, '\"CrumbStore\\\":\\{\\\"crumb\\\":\\\"[[:graph:]]+?\\\"\\}')
crumb <- unlist(strsplit(crumbled,split='"'))[6]
#test them
testurl <- paste0("https://query1.finance.yahoo.com/v7/finance/download/GOOG?period1=1451606400&period2=1483228800&interval=1d&events=history&crumb=",crumb)
scraped <- GET(testurl, config(cookie= cookie))
status <- status_code(scraped)
tries = tries + 1
}
if (status != 200){
message("ERROR: Couldn't access Yahoo after 5 tries")
}
if (status == 401){
message("ERROR: The cookie/crumb scrape didn't work... Fucking yahoo...")
}
if (verbose == TRUE){
message("Grabbing Stock data... This takes a while for long stocklists")
}
stocksdf<- lapply(stocklist,function(x){
if (verbose == TRUE){
message(paste0("Downloading ",x))
}
capture <- paste0("https://query1.finance.yahoo.com/v7/finance/download/",x,"?period1=",start,"&period2=",end,"&interval=1d&events=history&crumb=",crumb)
scraped <- GET(capture, config(cookie= cookie))
#the content() call is loud so I suppress messages for now
suppressMessages(data.frame(content(scraped)))
})
names(stocksdf) <- stocklist
return(stocksdf)
}
AMEX <- stockhistoricals("AMEX", start_date = "2016-09-10")
stockhistoricals <- function(stocklist="GOOG", start_date="1970-01-01", end_date=as.Date(Sys.time()), verbose=TRUE){
## Usage stockhistoricals(stocklist, start_date, end_date, verbose=TRUE)
##    stocklist : A vector of stock tickers. Default = GOOG. Can also pass "NYSE", "NASDAQ", or "AMEX" to get those lists.
##    start_date : start date in format Year-month-day. Default = "1970-01-01"
##    end_date : end date in format Year-month-day. Default = system date
##    verbost : Logical. if F, suppresses messages
require(XML)
require(RCurl)
require(httr)
require(readr)
options(warn=-1)
if(stocklist=="NASDAQ" | stocklist=="NYSE" | stocklist=="AMEX"){
stocklist <- get_stocklists(stocklist)
stocklist <- stocklist$Symbol
}
options(warn=0)
#86400 is the conversion from days to minutes (how yahoo is counting time)
#t=1 is 1970-01-01. So any is 86400 minutes per day from 1970-01-01
start <- as.numeric(as.Date(start_date)-as.Date("1970-01-01"))* 86400
end <- as.numeric(as.Date(end_date)-as.Date("1970-01-01"))* 86400
#grab a cookie. Try 5 times because sometimes yahoo puts fucking escape characters in the crumb
tries = 1
status = 1
while (tries < 5 && status !=200){
url <- paste0("https://finance.yahoo.com/quote/GOOG/history")
h <- handle(url)
res <- GET(handle = h)
response <- content(res, "text")
cookie <- unlist(strsplit(unlist(res$headers[5]),';'))[1]
#this gets a crumb pair to use. I hate regex
crumbled = stringr::str_extract(response, '\"CrumbStore\\\":\\{\\\"crumb\\\":\\\"[[:graph:]]+?\\\"\\}')
crumb <- unlist(strsplit(crumbled,split='"'))[6]
#test them
testurl <- paste0("https://query1.finance.yahoo.com/v7/finance/download/GOOG?period1=1451606400&period2=1483228800&interval=1d&events=history&crumb=",crumb)
scraped <- GET(testurl, config(cookie= cookie))
status <- status_code(scraped)
tries = tries + 1
}
if (status != 200){
message("ERROR: Couldn't access Yahoo after 5 tries")
}
if (status == 401){
message("ERROR: The cookie/crumb scrape didn't work... Fucking yahoo...")
}
if (verbose == TRUE){
message("Grabbing Stock data... This takes a while for long stocklists")
}
stocksdf<- lapply(stocklist,function(x){
if (verbose == TRUE){
message(paste0("Downloading ",x))
}
capture <- paste0("https://query1.finance.yahoo.com/v7/finance/download/",x,"?period1=",start,"&period2=",end,"&interval=1d&events=history&crumb=",crumb)
scraped <- GET(capture, config(cookie= cookie))
#the content() call is loud so I suppress messages for now
suppressMessages(data.frame(content(scraped)))
})
names(stocksdf) <- stocklist
return(stocksdf)
}
AMEX <- stockhistoricals("AMEX", start_date = "2016-09-10")
source("https://raw.githubusercontent.com/ScientistJake/StockScraper.R/master/StockScraper.R")
help("stop")
AMEX <- stockhistoricals("AMEX", start_date = "2016-09-10")
AMEXfilt <- AMEX[lapply(AMEX,length)==7]
AMEX[lapply(AMEX,length)!=7]
AMEX_symbols <-get_stocklists("AMEX")
View(AMEX_symbols)
AMEX_symbols[5]
AMEX_symbols[5,]
AMEX_symbols$Symbol[5,]
AMEX_symbols$Symbol[5]
library(quantmod)
getSymbols("NSPR.WS", verbose=T,src="yahoo")
stockhistoricals("NSPR.WS", start_date = "2016-09-10")
stockhistoricals("NSPR.WS", start_date = "2016-09-10")
stockhistoricals("NSPR-WS", start_date = "2016-09-10")
shiny::runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
shiny::runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
shiny::runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
runApp('Documents/Github_repos/Shiny_F1')
help(scale_fill_manual)
??scale_fill_manual
library(ggplot2)
shiny::runApp('Documents/Github_repos/Shiny_F1')
setwd("~/Documents/Github_repos/Shiny_F1")
con = dbConnect(RSQLite::SQLite(), dbname="ergast.sqlite")
drivers <- dbGetQuery(con, "SELECT * FROM drivers")
drivers
Currentdrivers <- fromJSON("http://ergast.com/api/f1/2017/drivers/.json?limit=2500")
parseCurrdrivers= data.frame(name=character())
Currdriversresults <- lapply(1:as.numeric(Currentdrivers[[1]][6][1]), function(i){
id <- Currentdrivers[[1]][[7]][[3]][[1]][i]
firstname <- Currentdrivers[[1]][[7]][[3]][[5]][i]
lastname <- toupper(Currentdrivers[[1]][[7]][[3]][[6]][i])
parseCurrdrivers=rbind(parseCurrdrivers,data.frame(
paste0(lastname,", ",firstname),
id)
)
names(parseCurrdrivers) = NULL
parseCurrdrivers
})
CurrDrivers <- unlist(lapply(Currdriversresults, '[[', 1))
CurrDrivers
races <- dbGetQuery(con, "SELECT * FROM races")
View(races)
lapTimes <- dbGetQuery(con, paste0("SELECT * FROM lapTimes where ((raceId = ",paste(as.numeric(973[1,1]),sep=""),"))"))
lapTimes <- dbGetQuery(con, paste0("SELECT * FROM lapTimes where ((raceId = ",paste(as.numeric(973),sep=""),"))"))
laptimes_drivers <- merge(lapTimes,drivers, by.x='driverId',by.y='driverId')
laptimes_drivers
View(laptimes_drivers)
View(drivers)
results <- dbGetQuery(con, paste0("SELECT * FROM results where ((driverId = ",paste(as.numeric(1),sep=""),"))"))
View(results)
View(laptimes_drivers)
